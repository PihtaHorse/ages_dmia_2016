{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_urls = pd.read_table('url_domain_train', names=['user_id', 'url', 'visits_count'])\n",
    "test_urls = pd.read_table('url_domain_test', names=['user_id', 'url', 'visits_count'])\n",
    "\n",
    "train_titles = pd.read_table('title_unify_train', names=['user_id', 'title', 'visits_count'])\n",
    "test_titles = pd.read_table('title_unify_test', names=['user_id', 'title', 'visits_count'])\n",
    "\n",
    "train_ages = pd.read_table('age_profile_train', names=['user_id', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_total_visits(df):\n",
    "    array = df['visits_count'].values\n",
    "    return array.sum()\n",
    "\n",
    "def count_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return len(array[array > 2])\n",
    "\n",
    "def count_sum_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array > 2])\n",
    "\n",
    "def count_sum_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array == 1])\n",
    "\n",
    "def count_visits_std(df):\n",
    "    array = df['visits_count'].values\n",
    "    return np.std(array)\n",
    "\n",
    "def count_visits_max(df):\n",
    "    array = df['visits_count'].values\n",
    "    return max(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activity_metrics = [count_total_visits,\n",
    "                    count_not_ones,\n",
    "                    count_sum_ones,\n",
    "                    count_sum_not_ones,\n",
    "                    count_visits_std,\n",
    "                    count_visits_max]\n",
    "\n",
    "def get_activitis(urls, titles, users_ids):\n",
    "    index = pd.Index(data=users_ids, name='user_id')\n",
    "    X = pd.DataFrame(index=index) \n",
    "    \n",
    "    activitis = [urls.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    activitis += [titles.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    \n",
    "    X = pd.concat(activitis, axis=1, join_axes=[users_ids])\n",
    "    X.fillna(0, inplace=True)\n",
    "    \n",
    "    return X.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Создадим scaler, который затем будет стандартизировать фичи \"активность\" для тренировачного и тестового сета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_activitis = get_activitis(train_urls, train_titles, train_ages['user_id'].values)\n",
    "activitis_scaler = preprocessing.MinMaxScaler()\n",
    "activitis_scaler = activitis_scaler.fit(train_activitis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Адреса(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_cutter(url):\n",
    "    *other_domains, upper_domain, lower_domain = url.split('.')\n",
    "    return '.'.join([upper_domain, lower_domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_users_urls(urls, users_ids):\n",
    "    urls_grouped = urls.groupby('user_id')\n",
    "    users_urls = urls_grouped.apply(lambda g: g['url'])\n",
    "    users_urls = [users_urls[user_id].values if user_id in users_urls else [] for user_id in users_ids]\n",
    "    users_urls = [[url_cutter(url) for url in user_urls] for user_urls in users_urls]\n",
    "    users_urls = [list(np.unique(user_urls)) for user_urls in users_urls]\n",
    "    \n",
    "    return users_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заголовки(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_users_titles(titles, users_ids):\n",
    "    words = titles.groupby('user_id').apply(lambda g: g['title'])\n",
    "    return [' '.join(words[user_id].values) if user_id in words else '' for user_id in users_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка всех фичей в одну матрицу признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def space_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "count_vectorizer_urls =  CountVectorizer(analyzer=\"word\",  lowercase=False, max_features=1000,\n",
    "                                         tokenizer=space_tokenizer, preprocessor=lambda l: ' '.join(l))\n",
    "\n",
    "count_vectorizer_titles =  CountVectorizer(analyzer=\"word\",  lowercase=False, max_features=5000,\n",
    "                                           tokenizer=space_tokenizer, stop_words=stopwords.words('russian'))\n",
    "\n",
    "count_vectorizer_urls = count_vectorizer_urls.fit(get_users_urls(train_urls, train_ages['user_id'].values))\n",
    "\n",
    "count_vectorizer_titles = count_vectorizer_titles.fit(get_users_titles(train_titles, train_ages['user_id'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_X(urls, titles, users_ids):\n",
    "    users_activitis = get_activitis(urls, titles, users_ids)\n",
    "    users_activitis = csr_matrix(activitis_scaler.transform(users_activitis))\n",
    "    users_urls = count_vectorizer_urls.transform(get_users_urls(urls, users_ids))\n",
    "    usesr_titles = count_vectorizer_titles.transform(get_users_titles(titles, users_ids))\n",
    "    return hstack((users_activitis, users_urls, usesr_titles)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train_ages['age'].values\n",
    "X_train = make_X(train_urls, train_titles, train_ages['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = pd.read_csv('sample_submission.csv', index_col='Id')\n",
    "X_test = make_X(test_urls, test_titles, Y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn_model(*, path_to_hdf5=False):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(800, input_dim=X_train.shape[1], activation='sigmoid')) #400\n",
    "    #model.add(Dense(50, activation='sigmoid'))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    if path_to_hdf5:\n",
    "        model.load_weights(path_to_hdf5)\n",
    "        \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "filepath=\"weights_{epoch:02d}_{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выставляем всего две эпохи, т.к. сеть переобучается уже после первой эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 800)           4810400     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             801         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,811,201\n",
      "Trainable params: 4,811,201\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/2\n",
      "Epoch 00000: val_loss improved from inf to 121.90729, saving model to weights_00_121.91.hdf5\n",
      "1200s - loss: 144.6854 - val_loss: 121.9073\n",
      "Epoch 2/2\n",
      "Epoch 00001: val_loss did not improve\n",
      "1387s - loss: 128.5880 - val_loss: 125.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0096173c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model()\n",
    "\n",
    "neural_network.fit(X_train, Y_train, nb_epoch=2, batch_size=20,\n",
    "                   validation_split=0.1, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выберем веса с самым лучшим результатом на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_3 (Dense)                  (None, 800)           4810400     dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             801         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,811,201\n",
      "Trainable params: 4,811,201\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_model(path_to_hdf5=\"weights_00_121.91.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_test = model.predict(X_test)\n",
    "Y_test['age'] = y_predicted_test\n",
    "Y_test.to_csv('ans.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
