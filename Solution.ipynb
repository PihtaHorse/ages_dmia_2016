{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "import pymorphy2\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_urls = pd.read_table('url_domain_train', names=['user_id', 'url', 'visits_count'])\n",
    "test_urls = pd.read_table('url_domain_test', names=['user_id', 'url', 'visits_count'])\n",
    "\n",
    "train_titles = pd.read_table('title_unify_train', names=['user_id', 'title', 'visits_count'])\n",
    "test_titles = pd.read_table('title_unify_test', names=['user_id', 'title', 'visits_count'])\n",
    "\n",
    "train_ages = pd.read_table('age_profile_train', names=['user_id', 'age'])\n",
    "Y_train = train_ages['age'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_total_visits(df):\n",
    "    array = df['visits_count'].values\n",
    "    return array.sum()\n",
    "\n",
    "def count_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return len(array[array > 2])\n",
    "\n",
    "def count_sum_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array > 2])\n",
    "\n",
    "def count_sum_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array == 1])\n",
    "\n",
    "def count_visits_std(df):\n",
    "    array = df['visits_count'].values\n",
    "    return np.std(array)\n",
    "\n",
    "def count_visits_max(df):\n",
    "    array = df['visits_count'].values\n",
    "    return max(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activity_metrics = [count_total_visits,\n",
    "                    count_not_ones,\n",
    "                    count_sum_ones,\n",
    "                    count_sum_not_ones,\n",
    "                    count_visits_std,\n",
    "                    count_visits_max]\n",
    "\n",
    "def get_activitis(urls, titles, users_ids):\n",
    "    index = pd.Index(data=users_ids, name='user_id')\n",
    "    X = pd.DataFrame(index=index) \n",
    "    \n",
    "    activitis = [urls.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    activitis += [titles.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    \n",
    "    X = pd.concat(activitis, axis=1, join_axes=[users_ids])\n",
    "    X.fillna(0, inplace=True)\n",
    "    \n",
    "    return X.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Создадим scaler, который затем будет стандартизировать фичи \"активность\" для тренировачного и тестового сета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_activitis = get_activitis(train_urls, train_titles, train_ages['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activitis_scaler = preprocessing.MinMaxScaler()\n",
    "activitis_scaler = activitis_scaler.fit(train_activitis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_activitis = activitis_scaler.transform(train_activitis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Адреса(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_cutter_all_domains_apart(url):\n",
    "    return url.replace('.', ' ')\n",
    "\n",
    "def url_cutter_all_domains_together(url):\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_users_urls(urls, users_ids, *, unique=False, url_cutter=url_cutter_all_domains_apart):\n",
    "    urls_grouped = urls.groupby('user_id')\n",
    "    \n",
    "    users_urls = {user_id: [item for item in g[['url', 'visits_count']].values] \n",
    "                  for user_id, g in urls_grouped}\n",
    "    \n",
    "    users_urls = {user_id: ' '.join([' '.join([url_cutter(url) for _ in range(count)]) for url, count in item]) \n",
    "                  for user_id, item in users_urls.items()}\n",
    "    \n",
    "    if unique:\n",
    "        users_urls = {user_id: ' '.join(np.unique(user_urls.split(' '))) \n",
    "                      for user_id, user_urls in users_urls.items()}\n",
    "    \n",
    "    users_urls = [users_urls[user_id] if user_id in users_urls else '' for user_id in users_ids]\n",
    "    \n",
    "    return users_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_urls = get_users_urls(train_urls, train_ages['user_id'].values, unique=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заголовки(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_users_titles(titles, users_ids, *, unique=False):\n",
    "    words = titles.groupby('user_id').apply(lambda g: g['title'])\n",
    "    words = [' '.join(words[user_id].values) if user_id in words else '' for user_id in users_ids]\n",
    "    if unique:\n",
    "        words = [' '.join(np.unique(user_words.split(' '))) for user_words in words]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_titles = get_users_titles(train_titles, train_ages['user_id'].values, unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_parh = os.path.join(os.sep, os.path.abspath('/home/data/word2vec/russian/ruwikiruscorpora_0_300_20.bin'))\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_parh, binary=True)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "to_posible_tags = {'NOUN': 'NOUN',\n",
    "                   'ADJF': 'ADJ',\n",
    "                   'ADJS': 'ADJ',\n",
    "                   'COMP': 'ADV',\n",
    "                   'VERB': 'VERB',\n",
    "                   'INFN': 'VERB',\n",
    "                   'PRTF': 'PART',\n",
    "                   'PRTS': 'PART',\n",
    "                   'INTJ': 'INTJ',\n",
    "                   'CONJ': 'CCONJ',\n",
    "                   'PREP': 'ADP',\n",
    "                   'NUMR': 'NUM',\n",
    "                   'PRCL': 'PART',\n",
    "                   'NPRO': 'PRON'\n",
    "                  }\n",
    "\n",
    "def tag_bagofwords(bow):\n",
    "    bow = bow.split(' ')\n",
    "    tags = [morph.parse(word)[0].tag.POS for word in bow]\n",
    "    return ' '.join([word + '_' + to_posible_tags[tag] if tag in to_posible_tags else word + '_' + 'NOUN' \n",
    "                     for word, tag in zip(bow, tags)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tagged_words(sentences, with_pool=False):\n",
    "    if with_pool:\n",
    "        with Pool(24) as p:\n",
    "            users_sentences_tagged = p.map(tag_bagofwords, sentences)\n",
    "    else:\n",
    "        users_sentences_tagged = list(map(tag_bagofwords, sentences))\n",
    "    \n",
    "    return users_sentences_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_averaging(words):\n",
    "    mean = [w2v.syn0norm[w2v.vocab[word].index] for word in words.split(' ') if word in w2v.vocab]\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_averaged_w2v_words(sentences_tagged, with_pool=False):\n",
    "    if with_pool:\n",
    "        with Pool(24) as p:\n",
    "            sentences_averaged = p.map(word_averaging, sentences_tagged)\n",
    "    else:\n",
    "        sentences_averaged = list(map(word_averaging, sentences_tagged))\n",
    "    \n",
    "    return sentences_averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### получение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_titles_tagged = get_tagged_words(users_titles, with_pool=True)\n",
    "users_titles_averaged = get_averaged_w2v_words(users_titles_tagged, with_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка всех фичей в одну матрицу признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def space_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "count_vectorizer_urls = CountVectorizer(analyzer=\"word\", lowercase=False, max_features=3000, tokenizer=space_tokenizer)\n",
    "\n",
    "count_vectorizer_titles = CountVectorizer(analyzer=\"word\", lowercase=False, max_features=10000, tokenizer=space_tokenizer)\n",
    "\n",
    "count_vectorizer_urls = count_vectorizer_urls.fit(users_urls)\n",
    "\n",
    "count_vectorizer_titles = count_vectorizer_titles.fit(users_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_X(urls, titles, users_ids):\n",
    "    users_activitis = get_activitis(urls, titles, users_ids)\n",
    "    users_activitis_csr = csr_matrix(activitis_scaler.transform(users_activitis))\n",
    "    \n",
    "    users_urls = get_users_urls(urls, users_ids, unique=True)\n",
    "    users_urls_csr = count_vectorizer_urls.transform(users_urls)\n",
    "    \n",
    "    users_titles = get_users_titles(titles, users_ids, unique=False)\n",
    "    usesr_titles_csr = count_vectorizer_titles.transform(users_titles)\n",
    "    \n",
    "    users_titles_tagged = get_tagged_words(users_titles, with_pool=True)\n",
    "    users_titles_averaged = get_averaged_w2v_words(users_titles_tagged, with_pool=True)\n",
    "    users_titles_averaged_csr = csr_matrix(users_titles_averaged)\n",
    "    \n",
    "    return hstack((users_activitis_csr, users_urls_csr, usesr_titles_csr, users_titles_averaged_csr)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = train_ages['age'].values\n",
    "X_train = make_X(train_urls, train_titles, train_ages['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = pd.read_csv('sample_submission.csv', index_col='Id')\n",
    "X_test = make_X(test_urls, test_titles, Y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nn_model(*, path_to_hdf5=False, input_dim=None):\n",
    "    input_dim = X_train.shape[1] if not input_dim else input_dim\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.5, input_shape=(input_dim,)))\n",
    "    model.add(Dense(1200, activation='sigmoid', init='uniform')) #400\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    \n",
    "    if path_to_hdf5:\n",
    "        model.load_weights(path_to_hdf5)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "filepath=\"weights_{epoch:02d}_{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Простое среднее и нейронная сеть с \"мусорными\" фичами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.37134402238277"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.ones(Y_train.shape)*Y_train.mean(), Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Т.е. всё, что лучше чем 158.37 уже хорошо. Проверим с нейронной сетью на вход которой подадим \"мусорные фичи\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_1 (Dropout)              (None, 10)            0           dropout_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1200)          13200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 1200)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             1201        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 14,401\n",
      "Trainable params: 14,401\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/10\n",
      "21s - loss: 175.8614 - val_loss: 152.2739\n",
      "Epoch 2/10\n",
      "20s - loss: 161.4913 - val_loss: 152.9236\n",
      "Epoch 3/10\n",
      "20s - loss: 161.4510 - val_loss: 148.6919\n",
      "Epoch 4/10\n",
      "20s - loss: 161.6897 - val_loss: 147.1121\n",
      "Epoch 5/10\n",
      "20s - loss: 161.5459 - val_loss: 145.4078\n",
      "Epoch 6/10\n",
      "21s - loss: 161.5594 - val_loss: 148.7841\n",
      "Epoch 7/10\n",
      "20s - loss: 161.6033 - val_loss: 147.7565\n",
      "Epoch 8/10\n",
      "20s - loss: 161.4613 - val_loss: 149.2556\n",
      "Epoch 9/10\n",
      "20s - loss: 161.7057 - val_loss: 151.8334\n",
      "Epoch 10/10\n",
      "20s - loss: 161.6648 - val_loss: 146.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f59b09b00>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_garbage = np.random.randn(118679, 10)\n",
    "\n",
    "neural_network = get_nn_model(input_dim=10)\n",
    "neural_network.fit(X_garbage, Y_train, nb_epoch=10, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вместе"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выставляем всего две эпохи, т.к. сеть переобучается уже после первой эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_3 (Dropout)              (None, 13312)         0           dropout_input_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1200)          15975600    dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 1200)          0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             1201        dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 15,976,801\n",
      "Trainable params: 15,976,801\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 126.39728, saving model to weights_00_126.40.hdf5\n",
      "254s - loss: 200.2161 - val_loss: 126.3973\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 126.39728 to 122.83116, saving model to weights_01_122.83.hdf5\n",
      "251s - loss: 138.6017 - val_loss: 122.8312\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 122.83116 to 121.44441, saving model to weights_02_121.44.hdf5\n",
      "254s - loss: 133.9755 - val_loss: 121.4444\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 121.44441 to 120.52919, saving model to weights_03_120.53.hdf5\n",
      "250s - loss: 131.6510 - val_loss: 120.5292\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 120.52919 to 120.00857, saving model to weights_04_120.01.hdf5\n",
      "248s - loss: 130.0953 - val_loss: 120.0086\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "253s - loss: 129.0481 - val_loss: 120.2581\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "251s - loss: 128.1962 - val_loss: 120.6446\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 120.00857 to 119.69983, saving model to weights_07_119.70.hdf5\n",
      "252s - loss: 126.6503 - val_loss: 119.6998\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "254s - loss: 126.0824 - val_loss: 121.5020\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "252s - loss: 125.0477 - val_loss: 120.8618\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "251s - loss: 124.5735 - val_loss: 120.0382\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "253s - loss: 123.7203 - val_loss: 120.2258\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "249s - loss: 122.9479 - val_loss: 119.7789\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "251s - loss: 122.1244 - val_loss: 120.0727\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "252s - loss: 121.7367 - val_loss: 119.9982\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "252s - loss: 120.7572 - val_loss: 120.3060\n",
      "Epoch 17/20\n",
      "Epoch 00016: val_loss did not improve\n",
      "255s - loss: 120.2198 - val_loss: 120.5018\n",
      "Epoch 18/20\n",
      "Epoch 00017: val_loss did not improve\n",
      "253s - loss: 119.1791 - val_loss: 120.3363\n",
      "Epoch 19/20\n",
      "Epoch 00018: val_loss did not improve\n",
      "257s - loss: 118.5663 - val_loss: 120.4410\n",
      "Epoch 20/20\n",
      "Epoch 00019: val_loss did not improve\n",
      "257s - loss: 118.3165 - val_loss: 120.5585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f8e096860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model()\n",
    "\n",
    "neural_network.fit(X_train, Y_train, nb_epoch=20, batch_size=300,\n",
    "                   validation_split=0.1, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выберем веса с самым лучшим результатом на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_5 (Dropout)              (None, 13312)         0           dropout_input_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1200)          15975600    dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 1200)          0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1)             1201        dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 15,976,801\n",
      "Trainable params: 15,976,801\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_model(path_to_hdf5=\"weights_07_119.70.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted_test = model.predict(X_test)\n",
    "Y_test['age'] = y_predicted_test\n",
    "Y_test.to_csv('ans.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
