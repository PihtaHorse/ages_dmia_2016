{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "import pymorphy2\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_urls = pd.read_table('url_domain_train', names=['user_id', 'url', 'visits_count'])\n",
    "test_urls = pd.read_table('url_domain_test', names=['user_id', 'url', 'visits_count'])\n",
    "\n",
    "train_titles = pd.read_table('title_unify_train', names=['user_id', 'title', 'visits_count'])\n",
    "test_titles = pd.read_table('title_unify_test', names=['user_id', 'title', 'visits_count'])\n",
    "\n",
    "train_ages = pd.read_table('age_profile_train', names=['user_id', 'age'])\n",
    "Y_train = train_ages['age'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_total_visits(df):\n",
    "    array = df['visits_count'].values\n",
    "    return array.sum()\n",
    "\n",
    "def count_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return len(array[array > 2])\n",
    "\n",
    "def count_sum_not_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array > 2])\n",
    "\n",
    "def count_sum_ones(df):\n",
    "    array = df['visits_count'].values\n",
    "    return sum(array[array == 1])\n",
    "\n",
    "def count_visits_std(df):\n",
    "    array = df['visits_count'].values\n",
    "    return np.std(array)\n",
    "\n",
    "def count_visits_max(df):\n",
    "    array = df['visits_count'].values\n",
    "    return max(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activity_metrics = [count_total_visits,\n",
    "                    count_not_ones,\n",
    "                    count_sum_ones,\n",
    "                    count_sum_not_ones,\n",
    "                    count_visits_std,\n",
    "                    count_visits_max]\n",
    "\n",
    "def get_activitis(urls, titles, users_ids):\n",
    "    index = pd.Index(data=users_ids, name='user_id')\n",
    "    X = pd.DataFrame(index=index) \n",
    "    \n",
    "    activitis = [urls.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    activitis += [titles.groupby('user_id').apply(metric) for metric in activity_metrics]\n",
    "    \n",
    "    X = pd.concat(activitis, axis=1, join_axes=[users_ids])\n",
    "    X.fillna(0, inplace=True)\n",
    "    \n",
    "    return X.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Создадим scaler, который затем будет стандартизировать фичи \"активность\" для тренировачного и тестового сета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_activitis = get_activitis(train_urls, train_titles, train_ages['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_activitis).to_csv('train_activitis.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Загрузить уже посчитанное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_activitis = pd.read_csv('train_activitis.csv', header=None)\n",
    "train_activitis = train_activitis.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activitis_scaler = preprocessing.MinMaxScaler()\n",
    "activitis_scaler = activitis_scaler.fit(train_activitis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_activitis = activitis_scaler.transform(train_activitis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Адреса(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_cutter_all_domains_apart(url):\n",
    "    return url.replace('.', ' ')\n",
    "\n",
    "def url_cutter_all_domains_together(url):\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_users_urls(urls, users_ids, *, unique=False, url_cutter=url_cutter_all_domains_apart):\n",
    "    urls_grouped = urls.groupby('user_id')\n",
    "    \n",
    "    users_urls = {user_id: [item for item in g[['url', 'visits_count']].values] \n",
    "                  for user_id, g in urls_grouped}\n",
    "    \n",
    "    users_urls = {user_id: ' '.join([' '.join([url_cutter(url) for _ in range(count)]) for url, count in item]) \n",
    "                  for user_id, item in users_urls.items()}\n",
    "    \n",
    "    if unique:\n",
    "        users_urls = {user_id: ' '.join(np.unique(user_urls.split(' '))) \n",
    "                      for user_id, user_urls in users_urls.items()}\n",
    "    \n",
    "    users_urls = [users_urls[user_id] if user_id in users_urls else '' for user_id in users_ids]\n",
    "    \n",
    "    return users_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_urls = get_users_urls(train_urls, train_ages['user_id'].values, unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users_urls).to_csv('users_urls.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_urls = pd.read_csv('users_urls.csv', header=None).fillna('')\n",
    "users_urls = users_urls[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заголовки(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_users_titles(titles, users_ids, *, unique=False):\n",
    "    words = titles.groupby('user_id').apply(lambda g: g['title'])\n",
    "    words = [' '.join(words[user_id].values) if user_id in words else '' for user_id in users_ids]\n",
    "    if unique:\n",
    "        words = [' '.join(np.unique(user_words.split(' '))) for user_words in words]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_titles = get_users_titles(train_titles, train_ages['user_id'].values, unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users_titles).to_csv('users_titles.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_titles = pd.read_csv('users_titles.csv', header=None).fillna('')\n",
    "users_titles = users_titles[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_parh = os.path.join(os.sep, os.path.abspath('/home/data/word2vec/russian/ruwikiruscorpora_0_300_20.bin'))\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_parh, binary=True)\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "to_posible_tags = {'NOUN': 'NOUN',\n",
    "                   'ADJF': 'ADJ',\n",
    "                   'ADJS': 'ADJ',\n",
    "                   'COMP': 'ADV',\n",
    "                   'VERB': 'VERB',\n",
    "                   'INFN': 'VERB',\n",
    "                   'PRTF': 'PART',\n",
    "                   'PRTS': 'PART',\n",
    "                   'INTJ': 'INTJ',\n",
    "                   'CONJ': 'CCONJ',\n",
    "                   'PREP': 'ADP',\n",
    "                   'NUMR': 'NUM',\n",
    "                   'PRCL': 'PART',\n",
    "                   'NPRO': 'PRON'\n",
    "                  }\n",
    "\n",
    "def tag_bagofwords(bow):\n",
    "    bow = bow.split(' ')\n",
    "    tags = [morph.parse(word)[0].tag.POS for word in bow]\n",
    "    return ' '.join([word + '_' + to_posible_tags[tag] if tag in to_posible_tags else word + '_' + 'NOUN' \n",
    "                     for word, tag in zip(bow, tags)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tagged_words(sentences, with_pool=False):\n",
    "    if with_pool:\n",
    "        with Pool(24) as p:\n",
    "            users_sentences_tagged = p.map(tag_bagofwords, sentences)\n",
    "    else:\n",
    "        users_sentences_tagged = list(map(tag_bagofwords, sentences))\n",
    "    \n",
    "    return users_sentences_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_averaging(words):\n",
    "    mean = [w2v.syn0norm[w2v.vocab[word].index] for word in words.split(' ') if word in w2v.vocab]\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_averaged_w2v_words(sentences_tagged, with_pool=False):\n",
    "    if with_pool:\n",
    "        with Pool(24) as p:\n",
    "            sentences_averaged = p.map(word_averaging, sentences_tagged)\n",
    "    else:\n",
    "        sentences_averaged = list(map(word_averaging, sentences_tagged))\n",
    "    \n",
    "    return sentences_averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### получение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_titles_tagged = get_tagged_words(users_titles, with_pool=True)\n",
    "users_titles_averaged = get_averaged_w2v_words(users_titles_tagged, with_pool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(users_titles_averaged).to_csv('word2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users_titles_averaged = pd.read_csv('word2vec.csv').values[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка всех фичей в одну матрицу признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def space_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "count_vectorizer_urls = CountVectorizer(analyzer=\"word\", lowercase=False, max_features=3000, tokenizer=space_tokenizer)\n",
    "\n",
    "count_vectorizer_titles = CountVectorizer(analyzer=\"word\", lowercase=False, max_features=10000, tokenizer=space_tokenizer)\n",
    "\n",
    "count_vectorizer_urls = count_vectorizer_urls.fit(users_urls)\n",
    "\n",
    "count_vectorizer_titles = count_vectorizer_titles.fit(users_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_X(urls, titles, users_ids):\n",
    "    users_activitis = get_activitis(urls, titles, users_ids)\n",
    "    users_activitis_csr = csr_matrix(activitis_scaler.transform(users_activitis))\n",
    "    \n",
    "    users_urls = get_users_urls(urls, users_ids, unique=True)\n",
    "    users_urls_csr = count_vectorizer_urls.transform(users_urls)\n",
    "    \n",
    "    users_titles = get_users_titles(titles, users_ids, unique=False)\n",
    "    usesr_titles_csr = count_vectorizer_titles.transform(users_titles)\n",
    "    \n",
    "    users_titles_tagged = get_tagged_words(users_titles, with_pool=True)\n",
    "    users_titles_averaged = get_averaged_w2v_words(users_titles_tagged, with_pool=True)\n",
    "    users_titles_averaged_csr = csr_matrix(users_titles_averaged)\n",
    "    \n",
    "    return hstack((users_activitis_csr, users_urls_csr, usesr_titles_csr, users_titles_averaged_csr)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = train_ages['age'].values\n",
    "X_train = make_X(train_urls, train_titles, train_ages['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = pd.read_csv('sample_submission.csv', index_col='Id')\n",
    "X_test = make_X(test_urls, test_titles, Y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "def get_nn_model(*, path_to_hdf5=False, input_dim=None):\n",
    "    input_dim = X_train.shape[1] if not input_dim else input_dim\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.5, input_shape=(input_dim,)))\n",
    "    model.add(Dense(1200, activation='sigmoid', init='uniform')) #400\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    \n",
    "    #model = Sequential()\n",
    "    #model.add(Dense(800, input_dim=input_dim, activation='sigmoid', init='normal')) #400\n",
    "    #model.add(Dense(50, activation='sigmoid'))\n",
    "    #model.add(Dense(1, init='normal'))\n",
    "    \n",
    "    if path_to_hdf5:\n",
    "        model.load_weights(path_to_hdf5)\n",
    "    \n",
    "    #sgd = SGD(lr=0.1, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "filepath=\"weights_{epoch:02d}_{val_loss:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По отдельности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Простое среднее для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.37134402238277"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.ones(Y_train.shape)*Y_train.mean(), Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Т.е. всё, что лучше чем 158.37 уже хорошо. Проверим с нейронной сетью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_7 (Dense)                  (None, 800)           8800        dense_input_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             801         dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 9,601\n",
      "Trainable params: 9,601\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 183.8688 - val_loss: 148.7040\n",
      "Epoch 2/10\n",
      "3s - loss: 160.0696 - val_loss: 150.5265\n",
      "Epoch 3/10\n",
      "3s - loss: 160.0886 - val_loss: 152.6107\n",
      "Epoch 4/10\n",
      "3s - loss: 160.0561 - val_loss: 145.6133\n",
      "Epoch 5/10\n",
      "3s - loss: 160.0460 - val_loss: 150.6054\n",
      "Epoch 6/10\n",
      "3s - loss: 160.1208 - val_loss: 149.5315\n",
      "Epoch 7/10\n",
      "3s - loss: 160.0222 - val_loss: 146.9438\n",
      "Epoch 8/10\n",
      "3s - loss: 160.0915 - val_loss: 151.1336\n",
      "Epoch 9/10\n",
      "3s - loss: 160.0648 - val_loss: 144.8734\n",
      "Epoch 10/10\n",
      "3s - loss: 160.0958 - val_loss: 146.3644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7444297b38>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_garbage = np.random.randn(118679, 10)\n",
    "\n",
    "neural_network = get_nn_model(input_dim=10)\n",
    "neural_network.fit(X_garbage, Y_train, nb_epoch=10, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Активность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 800)           10400       dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             801         dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 11,201\n",
      "Trainable params: 11,201\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/10\n",
      "7s - loss: 178.0444 - val_loss: 146.2305\n",
      "Epoch 2/10\n",
      "5s - loss: 157.3652 - val_loss: 149.4566\n",
      "Epoch 3/10\n",
      "5s - loss: 157.0014 - val_loss: 144.2987\n",
      "Epoch 4/10\n",
      "4s - loss: 156.7646 - val_loss: 144.4196\n",
      "Epoch 5/10\n",
      "4s - loss: 156.5645 - val_loss: 145.0560\n",
      "Epoch 6/10\n",
      "4s - loss: 156.5094 - val_loss: 144.6329\n",
      "Epoch 7/10\n",
      "3s - loss: 156.4477 - val_loss: 147.4821\n",
      "Epoch 8/10\n",
      "4s - loss: 156.3058 - val_loss: 145.8020\n",
      "Epoch 9/10\n",
      "3s - loss: 156.2542 - val_loss: 142.5123\n",
      "Epoch 10/10\n",
      "3s - loss: 156.2000 - val_loss: 143.0375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04a8a4ef98>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model(input_dim=12)\n",
    "neural_network.fit(users_activitis, Y_train, nb_epoch=10, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_3 (Dense)                  (None, 800)           240800      dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             801         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 241,601\n",
      "Trainable params: 241,601\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/10\n",
      "9s - loss: 163.3792 - val_loss: 131.0405\n",
      "Epoch 2/10\n",
      "9s - loss: 143.4178 - val_loss: 129.6864\n",
      "Epoch 3/10\n",
      "9s - loss: 142.7881 - val_loss: 132.5494\n",
      "Epoch 4/10\n",
      "9s - loss: 142.3876 - val_loss: 129.1316\n",
      "Epoch 5/10\n",
      "9s - loss: 142.1308 - val_loss: 130.9337\n",
      "Epoch 6/10\n",
      "8s - loss: 141.9427 - val_loss: 128.6137\n",
      "Epoch 7/10\n",
      "8s - loss: 141.7869 - val_loss: 131.9195\n",
      "Epoch 8/10\n",
      "9s - loss: 141.7720 - val_loss: 128.5471\n",
      "Epoch 9/10\n",
      "8s - loss: 141.5750 - val_loss: 129.2637\n",
      "Epoch 10/10\n",
      "9s - loss: 141.3640 - val_loss: 129.6097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04b12c9668>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model(input_dim=300)\n",
    "neural_network.fit(users_titles_averaged, Y_train, nb_epoch=10, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### urls CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_9 (Dense)                  (None, 800)           2400800     dense_input_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             801         dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2,401,601\n",
      "Trainable params: 2,401,601\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/3\n",
      "44s - loss: 158.2331 - val_loss: 125.0033\n",
      "Epoch 2/3\n",
      "41s - loss: 133.6768 - val_loss: 122.7429\n",
      "Epoch 3/3\n",
      "41s - loss: 130.9200 - val_loss: 124.0325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04aaf2f4e0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model(input_dim=3000)\n",
    "neural_network.fit(count_vectorizer_urls.transform(users_urls).toarray(), Y_train,\n",
    "                   nb_epoch=3, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### titles CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_11 (Dense)                 (None, 800)           8000800     dense_input_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1)             801         dense_11[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 8,001,601\n",
      "Trainable params: 8,001,601\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/3\n",
      "142s - loss: 151.3229 - val_loss: 121.2775\n",
      "Epoch 2/3\n",
      "140s - loss: 126.4118 - val_loss: 125.7611\n",
      "Epoch 3/3\n",
      "141s - loss: 120.9628 - val_loss: 124.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04ad13bda0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network = get_nn_model(input_dim=10000)\n",
    "neural_network.fit(count_vectorizer_titles.transform(users_titles).toarray(), Y_train,\n",
    "                   nb_epoch=3, batch_size=50, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вместе"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выставляем всего две эпохи, т.к. сеть переобучается уже после первой эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_21 (Dropout)             (None, 13312)         0           dropout_input_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 1200)          15975600    dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 1200)          0           dense_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 1)             1201        dropout_22[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 15,976,801\n",
      "Trainable params: 15,976,801\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 106811 samples, validate on 11868 samples\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_loss did not improve\n",
      "95s - loss: 203.4103 - val_loss: 126.5212\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_loss did not improve\n",
      "95s - loss: 138.6887 - val_loss: 121.4775\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_loss did not improve\n",
      "94s - loss: 133.8777 - val_loss: 121.7224\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_loss did not improve\n",
      "95s - loss: 131.5886 - val_loss: 120.7077\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_loss did not improve\n",
      "95s - loss: 130.4121 - val_loss: 121.0281\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_loss did not improve\n",
      "96s - loss: 128.9828 - val_loss: 119.7447\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_loss did not improve\n",
      "95s - loss: 128.1931 - val_loss: 119.8510\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_loss did not improve\n",
      "95s - loss: 126.7157 - val_loss: 119.6252\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_loss did not improve\n",
      "95s - loss: 125.9505 - val_loss: 119.5017\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_loss did not improve\n",
      "95s - loss: 125.1590 - val_loss: 119.6479\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_loss did not improve\n",
      "95s - loss: 124.5657 - val_loss: 120.0077\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_loss did not improve\n",
      "95s - loss: 123.8009 - val_loss: 121.6811\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_loss did not improve\n",
      "95s - loss: 123.1850 - val_loss: 119.5665\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_loss did not improve\n",
      "95s - loss: 122.1774 - val_loss: 120.2026\n",
      "Epoch 15/30\n",
      "Epoch 00014: val_loss did not improve\n",
      "95s - loss: 121.7095 - val_loss: 119.9967\n",
      "Epoch 16/30\n",
      "Epoch 00015: val_loss did not improve\n",
      "95s - loss: 120.9235 - val_loss: 119.6363\n",
      "Epoch 17/30\n",
      "Epoch 00016: val_loss did not improve\n",
      "95s - loss: 120.4599 - val_loss: 120.0100\n",
      "Epoch 18/30\n",
      "Epoch 00017: val_loss did not improve\n",
      "95s - loss: 119.4875 - val_loss: 120.8675\n",
      "Epoch 19/30\n",
      "Epoch 00018: val_loss did not improve\n",
      "95s - loss: 119.1776 - val_loss: 120.0452\n",
      "Epoch 20/30\n",
      "Epoch 00019: val_loss did not improve\n",
      "95s - loss: 117.5018 - val_loss: 120.4391\n",
      "Epoch 21/30\n",
      "Epoch 00020: val_loss did not improve\n",
      "95s - loss: 116.9617 - val_loss: 120.6549\n",
      "Epoch 22/30\n",
      "Epoch 00021: val_loss did not improve\n",
      "94s - loss: 116.6698 - val_loss: 120.4249\n",
      "Epoch 23/30\n",
      "Epoch 00022: val_loss did not improve\n",
      "94s - loss: 115.6474 - val_loss: 120.8211\n",
      "Epoch 24/30\n",
      "Epoch 00023: val_loss did not improve\n",
      "94s - loss: 115.2174 - val_loss: 121.9328\n",
      "Epoch 25/30\n",
      "Epoch 00024: val_loss did not improve\n",
      "95s - loss: 113.9987 - val_loss: 120.9104\n",
      "Epoch 26/30\n"
     ]
    }
   ],
   "source": [
    "neural_network = get_nn_model()\n",
    "\n",
    "neural_network.fit(X_train, Y_train, nb_epoch=30, batch_size=300,\n",
    "                   validation_split=0.1, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Выберем веса с самым лучшим результатом на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_17 (Dropout)             (None, 13312)         0           dropout_input_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 1200)          15975600    dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 1200)          0           dense_28[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 1)             1201        dropout_18[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 15,976,801\n",
      "Trainable params: 15,976,801\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_model(path_to_hdf5=\"weights_43_119.76.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted_test = model.predict(X_test)\n",
    "Y_test['age'] = y_predicted_test\n",
    "Y_test.to_csv('ans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
